{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import networkx as nx\n",
    "import os\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, Counter\n",
    "\n",
    "# Function to parse XPDL and order transitions correctly\n",
    "def parse_xpdl(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ns = {'xpdl': 'http://www.wfmc.org/2009/XPDL2.2'}\n",
    "    \n",
    "    # Lists\n",
    "    pools = []\n",
    "    lanes = []\n",
    "    activities = []\n",
    "    data_objects = []\n",
    "    transitions = []  # Now using a list with manual duplicate check\n",
    "    associations = []\n",
    "    message_flows = []\n",
    "    activity_id_to_name = {}  # Mapping from activity ID to name\n",
    "    id_to_activity = {}  # Store activity details for reference\n",
    "    \n",
    "    # Parse Pools\n",
    "    for pool in root.findall('.//xpdl:Pool', ns):\n",
    "        pool_name = pool.get('Name')\n",
    "        if pool_name:\n",
    "            pools.append({'name': pool_name})\n",
    "    \n",
    "    # Parse Lanes\n",
    "    for lane in root.findall('.//xpdl:Lane', ns):\n",
    "        lane_name = lane.get('Name')\n",
    "        if lane_name:\n",
    "            lanes.append({'name': lane_name})\n",
    "    \n",
    "    # Parse Activities\n",
    "    for activity in root.findall('.//xpdl:Activity', ns):\n",
    "        activity_id = activity.get('Id')\n",
    "        activity_name = activity.get('Name')\n",
    "        event = activity.find('.//xpdl:Event', ns)\n",
    "        route = activity.find('.//xpdl:Route', ns)\n",
    "        gateway_type = route.get('GatewayType') if route is not None else 'None'\n",
    "        is_start_event = False\n",
    "        is_end_event = False\n",
    "        \n",
    "        if event is not None:\n",
    "            if event.find('xpdl:StartEvent', ns) is not None:\n",
    "                activity_name = 'StartEvent'\n",
    "                is_start_event = True\n",
    "            elif event.find('xpdl:EndEvent', ns) is not None:\n",
    "                activity_name = 'EndEvent'\n",
    "                is_end_event = True\n",
    "\n",
    "        if activity_id:\n",
    "            activity_detail = {\n",
    "                'id': activity_id,\n",
    "                'name': activity_name,\n",
    "                'type': gateway_type,\n",
    "                'is_start_event': is_start_event,\n",
    "                'is_end_event': is_end_event,\n",
    "                'from': [],\n",
    "                'to': [],\n",
    "                'data_objects': []  # Initialize an empty list for attached data objects\n",
    "            }\n",
    "            id_to_activity[activity_id] = activity_detail\n",
    "            activity_id_to_name[activity_id] = activity_name if activity_name else activity_id\n",
    "\n",
    "    # Parse DataObjects and assign them to the corresponding Activity\n",
    "    for data_object in root.findall('.//xpdl:DataObject', ns):\n",
    "        data_id = data_object.get('Id')\n",
    "        data_name = data_object.get('Name')\n",
    "        if data_id and data_name:\n",
    "            for activity in id_to_activity.values():\n",
    "                activity['data_objects'].append({'id': data_id, 'name': data_name, 'type': 'DataObject'})\n",
    "\n",
    "    # Parse DataStores and assign them to the corresponding Activity\n",
    "    for data_store in root.findall('.//xpdl:DataStore', ns):\n",
    "        data_id = data_store.get('Id')\n",
    "        data_name = data_store.get('Name')\n",
    "        if data_id and data_name:\n",
    "            for activity in id_to_activity.values():\n",
    "                activity['data_objects'].append({'id': data_id, 'name': data_name, 'type': 'DataStore'})\n",
    "\n",
    "            \n",
    "    # Parse Transitions and create a transition map\n",
    "    transition_map = {}  # Map each source to its target(s)\n",
    "    reverse_transition_map = {}  # Map each target to its source(s)\n",
    "    for transition in root.findall('.//xpdl:Transition', ns):\n",
    "        source = transition.get('From')\n",
    "        target = transition.get('To')\n",
    "        if source and target:\n",
    "            if source not in transition_map:\n",
    "                transition_map[source] = []\n",
    "            transition_map[source].append(target)\n",
    "\n",
    "            if target not in reverse_transition_map:\n",
    "                reverse_transition_map[target] = []\n",
    "            reverse_transition_map[target].append(source)\n",
    "\n",
    "            # Update `from` and `to` references\n",
    "            if source in id_to_activity:\n",
    "                id_to_activity[source]['to'].append(target)\n",
    "            if target in id_to_activity:\n",
    "                id_to_activity[target]['from'].append(source)\n",
    "           \n",
    "    # Add Associations to Transitions\n",
    "    for association in root.findall('.//xpdl:Association', ns):\n",
    "        source = association.get('Source')\n",
    "        target = association.get('Target')\n",
    "        if source and target:\n",
    "            associations.append({'from': activity_id_to_name.get(source, source), 'to': activity_id_to_name.get(target, target)})\n",
    "\n",
    "    # Add Message Flows to Transitions\n",
    "    for message_flow in root.findall('.//xpdl:MessageFlow', ns):\n",
    "        source = message_flow.get('Source')\n",
    "        target = message_flow.get('Target')\n",
    "        if source and target:\n",
    "            message_flows.append({'from': activity_id_to_name.get(source, source), 'to': activity_id_to_name.get(target, target)})\n",
    "                \n",
    "    # Find the StartEvent node\n",
    "    start_node = next((act for act in id_to_activity.values() if act['is_start_event']), None)\n",
    "    if not start_node:\n",
    "        raise ValueError(\"No StartEvent found in the XPDL.\")\n",
    "\n",
    "    # Track visited nodes\n",
    "    visited = set()\n",
    "    ordered_transitions = []  # Store transitions in order\n",
    "\n",
    "    # Traverse the transitions in correct order using BFS\n",
    "    queue = deque([start_node['id']])\n",
    "    while queue:\n",
    "        current_id = queue.popleft()\n",
    "        if current_id in visited:\n",
    "            continue\n",
    "        visited.add(current_id)\n",
    "\n",
    "        if current_id in transition_map:\n",
    "            for target_id in transition_map[current_id]:\n",
    "                transition_entry = {\n",
    "                    'from': activity_id_to_name.get(current_id, current_id),\n",
    "                    'to': activity_id_to_name.get(target_id, target_id)\n",
    "                }\n",
    "\n",
    "                # **Manually check for duplicates before adding**\n",
    "                if transition_entry not in ordered_transitions:\n",
    "                    ordered_transitions.append(transition_entry)\n",
    "                queue.append(target_id)\n",
    "\n",
    "    # Detect missing transitions\n",
    "    missing_transitions = []\n",
    "    for activity in id_to_activity.keys():\n",
    "        if activity not in visited:\n",
    "            missing_transitions.append(activity)\n",
    "\n",
    "    # **Step 1: Reconstruct missing transitions using XPDL `from` and `to`**\n",
    "    if missing_transitions:\n",
    "        print(\"\\n⚠️ Missing transitions detected! Attempting to reconstruct...\")\n",
    "        for missing_id in missing_transitions:\n",
    "            missing_activity = id_to_activity[missing_id]\n",
    "            print(f\"  - {missing_activity['name']} ({missing_id}) is missing connections.\")\n",
    "\n",
    "            # Find the correct `from` node from the original XPDL transition list\n",
    "            if missing_id in reverse_transition_map:\n",
    "                correct_from = reverse_transition_map[missing_id][0]  # Take the first known connection\n",
    "                correct_from_name = activity_id_to_name.get(correct_from, correct_from)\n",
    "\n",
    "                transition_entry = {\n",
    "                    'from': correct_from_name,\n",
    "                    'to': activity_id_to_name.get(missing_id, missing_id)\n",
    "                }\n",
    "                if transition_entry not in ordered_transitions:  # Prevent duplicates\n",
    "                    ordered_transitions.append(transition_entry)\n",
    "                    print(f\"  ✅ Reconstructed missing transition: {correct_from_name} → {missing_activity['name']}\")\n",
    "\n",
    "            if missing_id in transition_map:\n",
    "                correct_to = transition_map[missing_id][0]  # Take the first known connection\n",
    "                correct_to_name = activity_id_to_name.get(correct_to, correct_to)\n",
    "\n",
    "                transition_entry = {\n",
    "                    'from': activity_id_to_name.get(missing_id, missing_id),\n",
    "                    'to': correct_to_name\n",
    "                }\n",
    "                if transition_entry not in ordered_transitions:  # Prevent duplicates\n",
    "                    ordered_transitions.append(transition_entry)\n",
    "                    print(f\"  ✅ Reconstructed missing transition: {missing_activity['name']} → {correct_to_name}\")\n",
    "\n",
    "    # **Step 3: Append Associations & Message Flows in Order**\n",
    "    ordered_transitions.extend(associations)\n",
    "    ordered_transitions.extend(message_flows)\n",
    "                    \n",
    "    print(\"Final Transitions:\", file_path, ordered_transitions)\n",
    "\n",
    "    return {\n",
    "        'pools': pools,\n",
    "        'lanes': lanes,\n",
    "        'activities': list(id_to_activity.values()),\n",
    "        'data_objects': data_objects,\n",
    "        'transitions': ordered_transitions,  # Use ordered transitions\n",
    "        'associations': associations,\n",
    "        'message_flows': message_flows\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_to_interstructural_with_order(parsed_data):\n",
    "    \"\"\"\n",
    "    Convert parsed XPDL data into a graph preserving the original process flow.\n",
    "    Activities and transitions are added in the correct order.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Map activity IDs to their details for quick access\n",
    "    activity_map = {activity['id']: activity for activity in parsed_data['activities']}\n",
    "\n",
    "    # Step 1: Add all activities first but connect them through transitions\n",
    "    print(\"parsed_data['transitions']\", parsed_data['transitions'])\n",
    "    for transition in parsed_data['transitions']:\n",
    "        source_id = transition['from']\n",
    "        target_id = transition['to']\n",
    "\n",
    "        # Ensure the source and target activities exist\n",
    "        source_activity = activity_map.get(source_id, {'name': source_id, 'type': 'Unknown'})\n",
    "        target_activity = activity_map.get(target_id, {'name': target_id, 'type': 'Unknown'})\n",
    "        print(\"source_id->\", source_id, \"|||\", source_activity)\n",
    "        print(\"target_id->\", target_id, \"|||\", target_activity)\n",
    "\n",
    "        if not source_activity['name']:\n",
    "            source_activity['name'] = str(source_activity[\"type\"]) + \"_\" + source_activity[\"id\"]\n",
    "            \n",
    "        if not target_activity['name']:\n",
    "            target_activity['name'] = str(target_activity[\"type\"]) + \"_\" + target_activity[\"id\"]\n",
    "        \n",
    "        # Add source activity if not already added\n",
    "        if not G.has_node(source_activity['name']):\n",
    "            G.add_node(source_activity['name'], label=source_activity['name'], type=source_activity.get('type', 'Activity'))\n",
    "            \n",
    "        # Add transition node\n",
    "        transition_node = f\"Transition_{source_activity['name']}_to_{target_activity['name']}\"\n",
    "        G.add_node(transition_node, label=transition_node, type=\"Transition\")    \n",
    "        \n",
    "        # Add target activity if not already added\n",
    "        if not G.has_node(target_activity['name']):\n",
    "            G.add_node(target_activity['name'], label=target_activity['name'], type=target_activity.get('type', 'Activity'))\n",
    "\n",
    "        # Connect source -> transition -> target\n",
    "        G.add_edge(source_activity['name'], transition_node)  # Source to Transition\n",
    "        G.add_edge(transition_node, target_activity['name'])  # Transition to Target\n",
    "\n",
    "    return G\n",
    "\n",
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    Converts a string into a frequency vector.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    return Counter(words)\n",
    "\n",
    "def cosine_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between two text strings.\n",
    "    \"\"\"\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "    # print(vector1, vector2)\n",
    "\n",
    "    intersection = set(vector1.keys()) & set(vector2.keys())\n",
    "    dot_product = sum([vector1[word] * vector2[word] for word in intersection])\n",
    "\n",
    "    magnitude1 = sqrt(sum([count ** 2 for count in vector1.values()]))\n",
    "    magnitude2 = sqrt(sum([count ** 2 for count in vector2.values()]))\n",
    "\n",
    "    if not magnitude1 or not magnitude2:\n",
    "        return 0.0\n",
    "\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def calculate_semantic_similarity(target_node, other_node, component):\n",
    "    \"\"\"\n",
    "    Calculates semantic similarity between two nodes using cosine similarity.\n",
    "    Missing sub-components are skipped, and the result is normalized by the total weight.\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjusted weights with 'label' and normalized to sum to 1\n",
    "    sub_components_weights = {\n",
    "        'pools': (['label'], [1.0]),\n",
    "        'lanes': (['label'], [1.0]),\n",
    "        'activities': (['label', 'type'], [0.5, 0.5]),\n",
    "        'data_objects': (['label', 'type'], [0.5, 0.5]),\n",
    "        'transitions': (['type'], [1.0]),  # Only check 'type' for transitions\n",
    "        'associations': (['label'], [1.0]),\n",
    "        'message_flows': (['label'], [1.0])\n",
    "    }\n",
    "\n",
    "    if component not in sub_components_weights:\n",
    "        return 0  # If the component type doesn't match, return 0 similarity\n",
    "\n",
    "    sub_components, weights = sub_components_weights[component]\n",
    "    similarities = []\n",
    "    existing_weights = 0  # To normalize only the used weights\n",
    "\n",
    "    # Calculate weighted cosine similarity for each sub-component\n",
    "    for sub_component, weight in zip(sub_components, weights):\n",
    "        target_value = target_node.get(sub_component, None)\n",
    "        other_value = other_node.get(sub_component, None)\n",
    "\n",
    "        # Skip if either value is missing\n",
    "        if target_value is None or other_value is None:\n",
    "            continue\n",
    "\n",
    "        # Apply cosine similarity for textual comparison\n",
    "        similarity = cosine_similarity(str(target_value), str(other_value))\n",
    "        similarities.append(weight * similarity)\n",
    "        existing_weights += weight  # Only sum up existing weights\n",
    "\n",
    "    # If no valid components were compared, return 0\n",
    "    if existing_weights == 0:\n",
    "        return 0\n",
    "\n",
    "    # Normalize by the sum of existing weights\n",
    "    return sum(similarities) / existing_weights\n",
    "\n",
    "\n",
    "def calculate_structural_similarity(target_node_data, other_node_data):\n",
    "    \"\"\"\n",
    "    Menghitung kemiripan struktural antara dua node menggunakan Graph Edit Distance (GED).\n",
    "    - Cost Insert, Delete, Substitusi = 1\n",
    "    - Jika tipe node berbeda → Cost 2 (karena harus delete + insert)\n",
    "    - Jika target node missing → Cost 1 (karena hanya perlu insert)\n",
    "    - Jika tipe sama tapi kontennya beda → Cost 1 (karena hanya perlu subtitusi)\n",
    "    \n",
    "    Hasil Similarity:\n",
    "    - Cost 0 → Similarity = 1.0 (100%)\n",
    "    - Cost 1 → Similarity = 0.5 (50%)\n",
    "    - Cost 2 → Similarity = 0.0 (0%)\n",
    "    \"\"\"\n",
    "    \n",
    "    cost_insert = 1\n",
    "    cost_delete = 1\n",
    "    cost_substitute = 1\n",
    "    \n",
    "    # Ambil tipe dan konten dari kedua node\n",
    "    target_type = target_node_data.get('type', 'Unknown')\n",
    "    other_type = other_node_data.get('type', 'Unknown')\n",
    "    \n",
    "    target_label = target_node_data.get('label', 'Unknown')\n",
    "    other_label = other_node_data.get('label', 'Unknown')\n",
    "\n",
    "    # Jika salah satu node hilang\n",
    "    if not target_node_data or not other_node_data:\n",
    "        cost = cost_insert\n",
    "    # Jika tipe berbeda, maka butuh delete + insert\n",
    "    elif target_type != other_type:\n",
    "        cost = cost_delete + cost_insert  # Total cost = 2\n",
    "    # Jika tipe sama tetapi label berbeda, hanya perlu subtitusi\n",
    "    elif target_label != other_label:\n",
    "        cost = cost_substitute\n",
    "    # Jika sama persis, tidak ada cost\n",
    "    else:\n",
    "        cost = 0\n",
    "\n",
    "    # Konversi cost ke similarity\n",
    "    if cost == 0:\n",
    "        similarity = 1.0  # 100% similarity\n",
    "    elif cost == 1:\n",
    "        similarity = 0.5  # 50% similarity\n",
    "    else:\n",
    "        similarity = 0.0  # 0% similarity\n",
    "\n",
    "    print(f\"Cost: {cost}, Structural Similarity: {similarity}\")\n",
    "    \n",
    "    return similarity, cost\n",
    "\n",
    "\n",
    "def detect_missing_nodes_by_count(graph):\n",
    "    \"\"\"\n",
    "    Detects missing nodes in the graph based on their count.\n",
    "    A node is considered missing if it does not appear at least twice in the graph.\n",
    "    \"\"\"\n",
    "    node_counts = {}\n",
    "    \n",
    "    # Count occurrences of each node in edges (both source and target)\n",
    "    for source, target in graph.edges():\n",
    "        node_counts[source] = node_counts.get(source, 0) + 1\n",
    "        node_counts[target] = node_counts.get(target, 0) + 1\n",
    "\n",
    "    # Identify nodes with less than 2 occurrences\n",
    "    missing_nodes = [node for node, count in node_counts.items() if count < 2 and node not in (\"StartEvent\", \"EndEvent\")]\n",
    "    # print(\"node_counts:\", node_counts)\n",
    "    # print(\"Missing Node:\", missing_nodes)\n",
    "    return missing_nodes\n",
    "\n",
    "\n",
    "def calculate_combined_similarity_with_ged(target_graph, target_node, other_graph, other_node, component):\n",
    "    \"\"\"\n",
    "    Menghitung kemiripan kombinasi (semantic + structural) antar node.\n",
    "    - **Semantic similarity** dihitung dengan cosine similarity.\n",
    "    - **Structural similarity** dihitung dengan Graph Edit Distance (GED).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ambil data node\n",
    "    target_node_data = target_node[1]\n",
    "    other_node_data = other_node[1]\n",
    "\n",
    "    # Hitung kemiripan semantic\n",
    "    semantic_similarity = calculate_semantic_similarity(target_node_data, other_node_data, component)\n",
    "\n",
    "    # Hitung kemiripan structural menggunakan GED\n",
    "    structural_similarity, cost = calculate_structural_similarity(target_node_data, other_node_data)\n",
    "    \n",
    "    print(f\"Semantic Similarity: {semantic_similarity}, Structural Similarity: {structural_similarity}\")\n",
    "\n",
    "    # Kombinasikan dengan bobot yang sama (50% semantic, 50% structural)\n",
    "    return semantic_similarity, structural_similarity, cost\n",
    "\n",
    "\n",
    "def compare_bpmn_with_missing_node_detection(target_data, other_data_list):\n",
    "    \"\"\"\n",
    "    Membandingkan target graph node-by-node dengan graph lain,\n",
    "    menggunakan GED Greedy untuk structural similarity dan semantic similarity,\n",
    "    serta mendeteksi missing nodes langsung dari target graph.\n",
    "    \"\"\"\n",
    "    target_graph = convert_to_interstructural_with_order(target_data)\n",
    "    target_nodes = list(target_graph.nodes(data=True))\n",
    "    \n",
    "    best_graph = None\n",
    "    best_graph_name = None\n",
    "    best_similarity = 0\n",
    "    best_semantic = 0\n",
    "    best_structural = 0\n",
    "    best_total_cost = 0\n",
    "    best_missing_nodes = []\n",
    "\n",
    "    # Detect missing nodes from the target graph\n",
    "    target_missing_nodes = detect_missing_nodes_by_count(target_graph)\n",
    "    print(\"Target Missing Nodes:\", target_missing_nodes)\n",
    "\n",
    "    for other_data in other_data_list:\n",
    "        current_missing_nodes = []\n",
    "        print(f\"\\nComparing with: {other_data['file_name']}\")\n",
    "        other_graph = convert_to_interstructural_with_order(other_data)\n",
    "        other_nodes = list(other_graph.nodes(data=True))\n",
    "\n",
    "        print(f\"{other_data['file_name']} Nodes:\", list(other_graph.nodes()))\n",
    "\n",
    "        total_semantic_similarity_graph = 0\n",
    "        total_structural_similarity_graph = 0\n",
    "        total_cost_graph = 0\n",
    "        total_nodes_compared_graph = 0\n",
    "        total_similarity = 0\n",
    "        valid = True\n",
    "\n",
    "        i, j = 0, 0\n",
    "        while i < len(target_nodes) and j < len(other_nodes):\n",
    "            target_nc = target_nodes[i]\n",
    "            other_nc = other_nodes[j]\n",
    "\n",
    "            component_type = 'activities' if target_nc[1].get('type') != 'Transition' else 'transitions'\n",
    "\n",
    "            if target_nc[0] in target_missing_nodes:\n",
    "                print(f\"Missing node detected: {target_nc[0]} - Reason: Consecutive non-Transition nodes\")\n",
    "\n",
    "                match_found = False\n",
    "                for k in range(j, len(other_nodes)):\n",
    "                    other_nc_next = other_nodes[k]\n",
    "                    semantic_similarity, structural_similarity, cost = calculate_combined_similarity_with_ged(\n",
    "                        target_graph, target_nc, other_graph, other_nc_next, component_type\n",
    "                    )\n",
    "                    \n",
    "                    total_cost_graph += cost\n",
    "\n",
    "                    combined_similarity_next = 0.5 * semantic_similarity + 0.5 * structural_similarity\n",
    "\n",
    "                    print(f\"{target_nc[0]} vs {other_nc_next[0]}: {combined_similarity_next}\")\n",
    "\n",
    "                    if combined_similarity_next > 0.5:\n",
    "                        print(f\"Match {target_nc[0]} vs {other_nc_next[0]}: {combined_similarity_next}\")\n",
    "                        match_found = True\n",
    "                        total_similarity += combined_similarity_next\n",
    "                        total_nodes_compared_graph += 1\n",
    "\n",
    "                        total_semantic_similarity_graph += semantic_similarity\n",
    "                        total_structural_similarity_graph += structural_similarity\n",
    "\n",
    "                        j = k  \n",
    "                        break\n",
    "                    else:\n",
    "                        current_missing_nodes.append(other_nc_next[0])\n",
    "\n",
    "                if not match_found:\n",
    "                    print(f\"Disqualifying graph: {other_data['file_name']} - Reason: Could not find match for missing node {target_nc[0]}\")\n",
    "                    valid = False\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                semantic_similarity, structural_similarity, cost = calculate_combined_similarity_with_ged(\n",
    "                    target_graph, target_nc, other_graph, other_nc, component_type\n",
    "                )\n",
    "                \n",
    "                total_cost_graph += cost\n",
    "\n",
    "                combined_similarity = 0.5 * semantic_similarity + 0.5 * structural_similarity\n",
    "\n",
    "                print(f\"Current: {target_nc[0]} vs {other_nc[0]} - Similarity: {combined_similarity}\")\n",
    "\n",
    "                if combined_similarity < 0.5:\n",
    "                    print(f\"Disqualifying graph: {other_data['file_name']} at node: {target_nc[0]} vs {other_nc[0]} - Similarity below threshold\")\n",
    "                    valid = False\n",
    "                    break\n",
    "\n",
    "                total_similarity += combined_similarity\n",
    "                total_nodes_compared_graph += 1\n",
    "\n",
    "                total_semantic_similarity_graph += semantic_similarity\n",
    "                total_structural_similarity_graph += structural_similarity\n",
    "\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "        if valid and total_nodes_compared_graph > 0:\n",
    "            avg_similarity = total_similarity / total_nodes_compared_graph\n",
    "            avg_semantic_similarity_graph = (total_semantic_similarity_graph / total_nodes_compared_graph)\n",
    "            avg_structural_similarity_graph = (total_structural_similarity_graph / total_nodes_compared_graph)\n",
    "\n",
    "            print(f\"\\nGraph: {other_data['file_name']} - Average Combined Similarity: {avg_similarity:.2f}\")\n",
    "            print(f\"Graph: {other_data['file_name']} - Total Semantic Similarity: {avg_semantic_similarity_graph}\")\n",
    "            print(f\"Graph: {other_data['file_name']} - Total Structural Similarity: {avg_structural_similarity_graph}\")\n",
    "            print(f\"Graph: {other_data['file_name']} - Total Cost: {total_cost_graph}\")\n",
    "\n",
    "            if avg_similarity > best_similarity:\n",
    "                best_graph = other_graph\n",
    "                best_graph_name = other_data.get('file_name', 'Unknown')\n",
    "                best_similarity = avg_similarity\n",
    "                best_semantic = avg_semantic_similarity_graph\n",
    "                best_structural = avg_structural_similarity_graph\n",
    "                best_missing_nodes = current_missing_nodes\n",
    "                best_cost = total_cost_graph\n",
    "\n",
    "    print(f\"\\nFinal Results: {target_data.get('file_name', 'Unknown')}\")\n",
    "    print(\"Best Graph:\", best_graph_name)\n",
    "    print(\"Best Similarity:\", best_similarity)\n",
    "    print(\"Best Semantic:\", best_semantic)\n",
    "    print(\"Best Structural:\", best_structural)\n",
    "    print(\"Best Cost:\", best_cost)\n",
    "    print(f\"Target Missing Nodes: {target_missing_nodes}\")\n",
    "    print(\"Matching nodes:\", best_missing_nodes)\n",
    "    \n",
    "    return best_graph, best_graph_name, target_graph\n",
    "\n",
    "\n",
    "\n",
    "# Visualize graph with updated labels\n",
    "def visualize_graph_with_labels(G, title=\"Graph Visualization\", file_name=\"Graph.png\"):\n",
    "    # Update node labels to include \"Start\" and \"End\"\n",
    "    # label_start_and_end_nodes(G)\n",
    "\n",
    "    # Use the updated labels for visualization\n",
    "    pos = nx.spring_layout(G)\n",
    "    labels = nx.get_node_attributes(G, 'label')\n",
    "    # Draw the graph with labels\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, labels=labels, \n",
    "            node_color='lightblue', edge_color='black', node_size=1000, font_size=12, font_weight='bold')\n",
    "    \n",
    "    # Set title and adjust layout\n",
    "    plt.title(title, fontsize=14)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = 'XPDL_3'\n",
    "    target_file = 'XPDL_3/Sales Transactions.xpdl'\n",
    "\n",
    "    # Parse target file\n",
    "    target_data = parse_xpdl(target_file)\n",
    "\n",
    "    # Parse other files\n",
    "    other_data_list = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if (file_name.endswith(\".xpdl\") or file_name.endswith(\".xml\"))  and file_name != os.path.basename(target_file):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            parsed_data = parse_xpdl(file_path)\n",
    "            parsed_data['file_name'] = file_name  # Add file name\n",
    "            other_data_list.append(parsed_data)\n",
    "        else:\n",
    "            target_data['file_name'] = file_name\n",
    "\n",
    "    # Compare BPMNs\n",
    "    best_graph, best_graph_name, target_graph = compare_bpmn_with_missing_node_detection(target_data, other_data_list)\n",
    "\n",
    "    # Visualize the main graph\n",
    "    visualize_graph_with_labels(target_graph, f\"Graph Visualization of Target XPDL: {target_file}\", f\"Visual/Target.png\")\n",
    "    if best_graph:\n",
    "        visualize_graph_with_labels(best_graph, f\"Graph Visualization of Best Match: {best_graph_name}\", f\"Visual/Match.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a0a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
